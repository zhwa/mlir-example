# Chapter 16: Nano LLM Serving - Complete Tutorial

---

## Table of Contents

1. [Introduction: The LLM Serving Challenge](#1-introduction-the-llm-serving-challenge)
2. [Architecture Overview](#2-architecture-overview)
3. [Phase 0: Request & Batch Abstractions](#3-phase-0-request--batch-abstractions)
4. [Phase 1: KV Cache Pool (C++)](#4-phase-1-kv-cache-pool-c)
5. [Phase 2: Prefill vs Decode Scheduling](#5-phase-2-prefill-vs-decode-scheduling)
6. [Phase 3: Chunked Prefill for Long Contexts](#6-phase-3-chunked-prefill-for-long-contexts)
7. [Phase 4: Radix Cache - The Key Innovation](#7-phase-4-radix-cache---the-key-innovation)
8. [Phase 5: Continuous Batching](#8-phase-5-continuous-batching)
9. [Phase 6: Complete Integration](#9-phase-6-complete-integration)
10. [Performance Analysis](#10-performance-analysis)
11. [Comparison with Production Systems](#11-comparison-with-production-systems)
12. [Exercises & Extensions](#12-exercises--extensions)

---

## 1. Introduction: The LLM Serving Challenge

### 1.1 What is LLM Serving?

When you use ChatGPT or Claude, you're interacting with a **serving system** that:
1. Receives your prompt (text input)
2. Runs the LLM model to generate a response
3. Streams tokens back to you in real-time
4. Handles thousands of concurrent users efficiently

**The Challenge**: LLMs are HUGE (billions of parameters) and SLOW (each token requires a full forward pass). How do we serve many users simultaneously without waiting forever?

### 1.2 Naive Approach: One Request at a Time

```python
# âŒ SLOW: Sequential processing
for user_request in requests:
    response = model.generate(user_request.prompt)
    send_response(user_request.user_id, response)
```

**Problem**: If each generation takes 5 seconds and you have 100 users, the last user waits **500 seconds** (8+ minutes)! ğŸ˜±

### 1.3 The Solution: Batching + Smart Scheduling

Modern serving systems use several techniques:

1. **Batching**: Process multiple requests simultaneously (GPU parallelism)
2. **Continuous Batching**: Add/remove requests dynamically as they finish
3. **KV Cache Reuse**: Don't recompute what you've already computed
4. **Memory Management**: Efficiently pack KV caches in limited GPU memory

**Result**: 10-100x higher throughput! ğŸš€

This chapter builds a **simplified serving engine** demonstrating these techniques.

---

## 2. Architecture Overview

### 2.1 System Design

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Python Layer (Serving Logic)                       â”‚
â”‚  - Request/Batch management                         â”‚
â”‚  - Radix cache (prefix matching tree)               â”‚
â”‚  - Scheduling (prefill/decode decisions)            â”‚
â”‚  - Continuous batching loop                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚ Python API calls
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  C++/MLIR Layer (Model Execution)                   â”‚
â”‚  - GPT model with MLIR JIT (from Chapter 14)        â”‚
â”‚  - KV cache pool (C++ memory management)            â”‚
â”‚  - Forward pass (JIT-compiled MLIR)                 â”‚
â”‚  - Pybind11 bindings                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.2 Why This Split?

- **Python**: High-level logic (scheduling, tree operations) - easy to develop/debug
- **C++/MLIR**: Performance-critical execution - fast matrix operations
- **Hybrid**: Best of both worlds!

### 2.3 Key Components We'll Build

| Component | Purpose | Language |
|-----------|---------|----------|
| Request/Batch | Data structures for requests | Python |
| KV Cache Pool | Memory management for attention | C++ |
| Prefill Manager | Schedule initial prompt processing | Python |
| Decode Manager | Schedule token generation | Python |
| Radix Cache | Automatic prefix sharing | Python |
| Continuous Batcher | Main serving loop | Python |
| Model Executor | Run GPT model | C++/MLIR |

---

## 3. Phase 0: Request & Batch Abstractions

### 3.1 Understanding Requests

A **request** represents one user's generation task:

```python
class Request:
    req_id: int              # Unique identifier
    prompt_tokens: List[int] # Input tokens [1, 2, 3, ...]
    max_tokens: int          # How many tokens to generate
    temperature: float       # Sampling randomness (0=greedy, 1=normal)

    # State tracking
    output_tokens: List[int] # Generated tokens so far
    kv_pages: List[int]      # Which KV cache pages we're using
    is_finished: bool        # Done generating?
```

**Example**:
```python
# User: "What is the capital of France?"
req = Request(
    req_id=1,
    prompt_tokens=[849, 318, 262, 3139, 286, 4881, 30],  # Tokenized
    max_tokens=20,
    temperature=0.7
)
```

### 3.2 Understanding Batches

A **batch** groups multiple requests for parallel processing:

```python
class Batch:
    requests: List[Request]
    is_prefill: bool           # Prefill or decode?
    input_ids: np.ndarray      # Tokens to process
    positions: np.ndarray      # Position indices for RoPE
    out_loc: List[int]         # Where to write outputs
```

### 3.3 Two Types of Batches

#### Prefill Batch: Process Initial Prompts
```python
# Multiple requests, variable prompt lengths
requests = [
    Request(prompt=[1, 2, 3, 4]),      # 4 tokens
    Request(prompt=[5, 6, 7]),         # 3 tokens
]

batch = Batch.from_prefill(requests)
# input_ids = [1, 2, 3, 4, 5, 6, 7]  (concatenated)
# positions = [0, 1, 2, 3, 0, 1, 2]   (per-request positions)
```

**Key**: Process ALL tokens of each prompt in parallel!

#### Decode Batch: Generate Next Tokens
```python
# All requests generate ONE token each
requests = [
    Request(output_tokens=[10, 20]),   # Next position: 2
    Request(output_tokens=[30]),        # Next position: 1
]

batch = Batch.from_decode(requests)
# input_ids = [last_token_1, last_token_2]  (one per request)
# positions = [2, 1]                         (current positions)
```

**Key**: Each request generates exactly one token in parallel!

### 3.4 Why Separate Prefill and Decode?

They have **completely different characteristics**:

| | Prefill | Decode |
|---|---------|--------|
| **Input Size** | Many tokens (10-1000s) | 1 token |
| **Compute** | High (process many tokens) | Low (1 token) |
| **Memory** | Low (no KV cache yet) | High (store all past KV) |
| **Batching** | Harder (variable lengths) | Easy (all 1 token) |

**Strategy**: Optimize each phase differently!

---

## 4. Phase 1: KV Cache Pool (C++)

### 4.1 What is KV Cache?

In transformer attention, each token needs to attend to all previous tokens:

```
Token 1: Attends to []
Token 2: Attends to [Token 1]
Token 3: Attends to [Token 1, Token 2]
Token 4: Attends to [Token 1, Token 2, Token 3]
```

Without caching, we'd recompute K and V for every token **every time**. Exponential cost! ğŸ˜±

**KV Cache** stores the Key and Value projections so we only compute them once:

```python
# Without cache: O(nÂ²) recomputation
for pos in range(seq_len):
    k = compute_keys(tokens[:pos+1])    # Recompute everything!
    v = compute_values(tokens[:pos+1])  # Recompute everything!
    output[pos] = attention(q[pos], k, v)

# With cache: O(n) - only compute new token
kv_cache = []
for pos in range(seq_len):
    k_new = compute_key(tokens[pos])     # Just this token!
    v_new = compute_value(tokens[pos])   # Just this token!
    kv_cache.append((k_new, v_new))
    output[pos] = attention(q[pos], kv_cache)
```

**Massive speedup**: 100x faster for long sequences!

### 4.2 The Memory Problem

For a GPT-style model:
```
KV Cache Size per Token = 2 Ã— num_layers Ã— num_heads Ã— head_dim Ã— 4 bytes

Example (GPT-3 scale):
= 2 Ã— 96 layers Ã— 96 heads Ã— 128 dim Ã— 4 bytes
= 9.4 MB per token! ğŸ˜±
```

For 100 users with 1000 tokens each = **940 GB** of KV cache! No GPU has that much memory.

### 4.3 Solution: Paged Memory (like OS virtual memory)

**Key Insight**: Borrow the idea from operating systems!

```
OS Virtual Memory          â†’    KV Cache Pages
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Physical pages (4KB)       â†’    Cache pages (256 tokens)
Page table mapping         â†’    Request â†’ Page mapping
Allocate/Free pages        â†’    Allocate/Free cache pages
```

**Benefits**:
1. **Fragmentation**: Pack multiple requests efficiently
2. **Sharing**: Multiple requests can share pages (we'll use this in Phase 4!)
3. **Flexibility**: Allocate/free dynamically

### 4.4 Implementation: C++ KV Cache Pool

```cpp
class KVCachePool {
    // Storage: One big array for all pages
    std::vector<std::vector<float>> k_cache_;  // [num_layers][total_cache_size]
    std::vector<std::vector<float>> v_cache_;  // [num_layers][total_cache_size]

    // Free page tracking
    std::set<int> free_pages_;  // {0, 1, 2, ..., num_pages-1}

    int num_pages_;
    int page_size_;  // Tokens per page

public:
    // Allocate pages for a request
    std::vector<int> allocate(int num_tokens) {
        int num_pages_needed = (num_tokens + page_size_ - 1) / page_size_;

        if (free_pages_.size() < num_pages_needed)
            throw std::runtime_error("Out of memory!");

        std::vector<int> allocated;
        for (int i = 0; i < num_pages_needed; i++) {
            int page = *free_pages_.begin();
            free_pages_.erase(free_pages_.begin());
            allocated.push_back(page);
        }
        return allocated;
    }

    // Free pages when request finishes
    void free(const std::vector<int>& pages) {
        for (int page : pages)
            free_pages_.insert(page);
    }
};
```

**Why C++?**
- Performance: Direct memory access, no Python overhead
- Efficiency: Tight control over memory layout
- Interop: Easy to call from Python via pybind11

### 4.5 Python Bindings

```python
# pybind11 wrapper
import ch16  # C++ module

class KVCachePool:
    def __init__(self, num_pages, page_size, num_layers, num_heads, head_dim):
        self._pool = ch16.KVCachePool(
            num_pages, page_size, num_layers, num_heads, head_dim
        )

    def allocate(self, num_tokens):
        return self._pool.allocate(num_tokens)

    def free(self, pages):
        self._pool.free(pages)
```

Now Python code can efficiently manage KV cache!

---

## 5. Phase 2: Prefill vs Decode Scheduling

### 5.1 The Scheduling Problem

You have 10 waiting requests. What do you do?

**Option 1**: Process all prefills first, then all decodes
- âŒ Problem: Decode requests starve (wait too long)

**Option 2**: Round-robin between prefill and decode
- âŒ Problem: Inefficient (constant switching)

**Option 3**: Smart scheduling based on resource availability
- âœ… Best: Balance throughput and latency!

### 5.2 Prefill Manager: FCFS with Token Budget

```python
class PrefillManager:
    def __init__(self, kv_pool, max_prefill_tokens=2048):
        self.kv_pool = kv_pool
        self.max_prefill_tokens = max_prefill_tokens

    def schedule(self, waiting_requests):
        """Select which prefills to run this step"""
        selected = []
        total_tokens = 0

        for req in waiting_requests:
            prompt_len = len(req.prompt_tokens)

            # Check token budget
            if total_tokens + prompt_len > self.max_prefill_tokens:
                break  # Too many tokens

            # Check memory
            pages_needed = (prompt_len + page_size - 1) // page_size
            if self.kv_pool.num_free_pages < pages_needed:
                break  # Out of memory

            # Allocate and select
            req.kv_pages = self.kv_pool.allocate(prompt_len)
            selected.append(req)
            total_tokens += prompt_len

        return Batch.from_prefill(selected)
```

**Key Ideas**:
1. **FCFS**: First-come-first-served for fairness
2. **Token Budget**: Limit batch size to avoid stalling decode
3. **Memory Check**: Don't OOM!

### 5.3 Decode Manager: Batch All Running Requests

```python
class DecodeManager:
    def __init__(self, max_batch_size=32):
        self.running_requests = []
        self.max_batch_size = max_batch_size

    def schedule(self):
        """Batch all running requests (up to max_batch_size)"""
        if not self.running_requests:
            return None

        # Remove finished requests
        self.running_requests = [
            req for req in self.running_requests 
            if not req.is_finished
        ]

        # Select up to max_batch_size
        selected = self.running_requests[:self.max_batch_size]

        return Batch.from_decode(selected)
```

**Key Ideas**:
1. **Simple**: All running requests decode together
2. **Efficient**: Decoding is cheap (1 token per request)
3. **Batch Size Limit**: GPU memory constraint

### 5.4 Coordinating Prefill and Decode

```python
# Main serving loop sketch
while has_pending_requests():
    # 1. Try to schedule prefill (if room)
    prefill_batch = prefill_mgr.schedule(waiting_requests)
    if prefill_batch:
        execute(prefill_batch)
        move_to_running(prefill_batch.requests)

    # 2. Schedule decode
    decode_batch = decode_mgr.schedule()
    if decode_batch:
        execute(decode_batch)

    # 3. Remove finished
    remove_finished_requests()
```

**Balance**: Prioritize decode (low latency) but admit new prefills when possible (high throughput).

---

## 6. Phase 3: Chunked Prefill for Long Contexts

### 6.1 The Long Context Problem

What if a user sends a **100,000 token** document?

**Naive approach**: Process all 100K tokens in one prefill batch
- âŒ Takes forever (seconds)
- âŒ Blocks all decode requests (everyone else waits)
- âŒ Requires huge memory spike

**Better approach**: **Chunk** the prompt into smaller pieces!

### 6.2 How Chunking Works

```python
# Long prompt: 100,000 tokens
prompt = [token_1, token_2, ..., token_100000]

# Chunk into 256-token pieces
chunks = [
    [token_1, ..., token_256],      # Chunk 1
    [token_257, ..., token_512],    # Chunk 2
    ...
    [token_99745, ..., token_100000] # Chunk N
]

# Process incrementally:
for chunk in chunks:
    process_prefill(chunk)
    # Between chunks: process decode batches!
```

**Benefits**:
1. **Fairness**: Decode requests don't starve
2. **Memory**: Smaller working set
3. **Flexibility**: Can schedule other requests between chunks

### 6.3 Implementation: ChunkedRequest

```python
class ChunkedRequest:
    def __init__(self, request, chunk_size=256):
        self.request = request
        self.chunk_size = chunk_size
        self.current_chunk_idx = 0

    @property
    def has_more_chunks(self):
        processed = self.current_chunk_idx * self.chunk_size
        return processed < len(self.request.prompt_tokens)

    def get_next_chunk(self):
        """Return next chunk of tokens"""
        start = self.current_chunk_idx * self.chunk_size
        end = min(start + self.chunk_size, 
                  len(self.request.prompt_tokens))
        self.current_chunk_idx += 1
        return self.request.prompt_tokens[start:end]
```

### 6.4 ChunkedPrefillManager: Round-Robin Scheduling

```python
class ChunkedPrefillManager:
    def __init__(self, kv_pool, max_chunk_size=256):
        self.kv_pool = kv_pool
        self.max_chunk_size = max_chunk_size
        self.chunked_requests = []

    def add_request(self, req):
        chunked = ChunkedRequest(req, self.max_chunk_size)
        self.chunked_requests.append(chunked)

    def schedule(self):
        """Round-robin: take one chunk from each request"""
        selected_chunks = []
        total_tokens = 0

        for chunked_req in self.chunked_requests:
            if not chunked_req.has_more_chunks:
                continue

            chunk = chunked_req.get_next_chunk()
            if total_tokens + len(chunk) <= MAX_BATCH_TOKENS:
                selected_chunks.append((chunked_req.request, chunk))
                total_tokens += len(chunk)

                # Allocate pages for this chunk
                pages = self.kv_pool.allocate(len(chunk))
                chunked_req.request.kv_pages.extend(pages)

        return Batch.from_chunks(selected_chunks)
```

**Key**: Round-robin ensures all long requests make progress (no starvation).

### 6.5 Example Timeline

```
Step 1: Process chunk from Request A (256 tokens)
Step 2: Decode for Requests X, Y, Z (3 tokens total)
Step 3: Process chunk from Request A (256 tokens)
Step 4: Decode for Requests X, Y, Z (3 tokens total)
Step 5: Process chunk from Request B (256 tokens)
Step 6: Decode for Requests X, Y, Z (3 tokens total)
...
```

**Result**: Long requests don't block short ones! ğŸ‰

---

## 7. Phase 4: Radix Cache - The Key Innovation

### 7.1 The Prefix Sharing Opportunity

Consider these requests:
```
Request 1: "What is the capital of France?"
Request 2: "What is the capital of Germany?"
Request 3: "What is the capital of Spain?"
Request 4: "What is the weather today?"
```

They share common prefixes:
```
"What is the " (shared by all 4)
"What is the capital of " (shared by 1, 2, 3)
```

**Key Insight**: If we already computed KV cache for "What is the capital of", we can **reuse it** for all three requests!

### 7.2 Quantifying the Savings

Without prefix sharing:
```
Request 1: Compute KV for 7 tokens
Request 2: Compute KV for 7 tokens
Request 3: Compute KV for 7 tokens
Total: 21 token computations
```

With prefix sharing:
```
Shared prefix "What is the capital of": Compute once (6 tokens)
Request 1: Compute "France?" (1 token)
Request 2: Compute "Germany?" (1 token)
Request 3: Compute "Spain?" (1 token)
Total: 9 token computations
```

**Savings**: 57% fewer computations! ğŸš€

In production systems (SGLang, vLLM), this gives **40-60% cache hit rates** in realistic workloads!

### 7.3 Data Structure: Radix Tree (Prefix Tree)

A **radix tree** is a tree where each path from root represents a sequence:

```
                   [root]
                     |
                 "What is the"
                /            \
          "capital of"    "weather"
          /    |    \          |
    "France" "Germany" "Spain"  "today"
```

**Properties**:
1. Shared prefixes = shared nodes
2. Each node stores KV cache pages for its token
3. Multiple requests can share the same node

### 7.4 Implementation: RadixNode

```python
class RadixNode:
    def __init__(self, token=None):
        self.token = token                      # Token ID
        self.children = {}                      # {token_id: RadixNode}
        self.kv_pages = []                      # KV cache pages for this token
        self.ref_count = 0                      # How many requests use this?
        self.last_access_time = 0.0             # For LRU eviction

    def add_child(self, token, kv_page):
        child = RadixNode(token)
        child.kv_pages = [kv_page]
        self.children[token] = child
        return child

    def get_child(self, token):
        return self.children.get(token, None)
```

### 7.5 Implementation: RadixCache

```python
class RadixCache:
    def __init__(self, kv_pool):
        self.root = RadixNode(token=None)
        self.kv_pool = kv_pool

    def match_prefix(self, tokens):
        """Find longest matching prefix in cache"""
        node = self.root
        matched_len = 0

        for i, token in enumerate(tokens):
            if token in node.children:
                node = node.children[token]
                matched_len = i + 1
            else:
                break  # No longer matching

        return matched_len, node

    def insert(self, tokens, kv_pages):
        """Insert a sequence into the cache"""
        node = self.root

        for token, page in zip(tokens, kv_pages):
            if token not in node.children:
                node = node.add_child(token, page)
            else:
                node = node.children[token]

            node.ref_count += 1
            node.last_access_time = time.time()
```

### 7.6 High-Level API: RadixCacheManager

```python
class RadixCacheManager:
    def __init__(self, kv_pool):
        self.cache = RadixCache(kv_pool)
        self.kv_pool = kv_pool

        # Statistics
        self.cache_hits = 0
        self.cache_misses = 0

    def get_or_allocate(self, tokens):
        """
        Main API: Get cached pages or allocate new ones

        Returns:
            (cached_len, new_pages)
        """
        # Step 1: Check cache for prefix
        cached_len, last_node = self.cache.match_prefix(tokens)

        if cached_len > 0:
            self.cache_hits += cached_len

        uncached_len = len(tokens) - cached_len

        if uncached_len == 0:
            # Everything cached!
            return cached_len, []

        # Step 2: Allocate pages for uncached suffix
        self.cache_misses += uncached_len
        new_pages = []

        for _ in range(uncached_len):
            page = self.kv_pool.allocate(1)  # 1 page per token
            new_pages.extend(page)

        # Step 3: Insert complete sequence into cache
        all_pages = self._get_cached_pages(tokens[:cached_len]) + new_pages
        self.cache.insert(tokens, all_pages)

        return cached_len, new_pages

    @property
    def cache_hit_rate(self):
        total = self.cache_hits + self.cache_misses
        return self.cache_hits / total if total > 0 else 0.0
```

### 7.7 LRU Eviction: What if Memory is Full?

When the cache fills up, we need to evict something. Use **LRU** (Least Recently Used):

```python
def evict_lru_leaf(self):
    """Find and evict the least recently used leaf node"""
    oldest_leaf = None
    oldest_time = float('inf')

    # DFS to find oldest leaf
    def find_oldest_leaf(node, parent):
        nonlocal oldest_leaf, oldest_time

        if node.is_leaf() and node.last_access_time < oldest_time:
            oldest_leaf = (node, parent)
            oldest_time = node.last_access_time

        for child in node.children.values():
            find_oldest_leaf(child, node)

    find_oldest_leaf(self.root, None)

    if oldest_leaf:
        node, parent = oldest_leaf
        # Free KV pages
        self.kv_pool.free(node.kv_pages)
        # Remove from tree
        parent.children.pop(node.token)
```

**Strategy**: Keep frequently accessed prefixes, evict old unused ones.

### 7.8 Real-World Impact

In production systems (SGLang):
- **Chat conversations**: "You are a helpful assistant..." shared across all messages
- **RAG systems**: Document context shared across multiple queries
- **Few-shot prompting**: Examples shared across variations

**Result**: 40-60% of tokens come from cache = 2-3x faster! ğŸš€

---

## 8. Phase 5: Continuous Batching

### 8.1 The Problem with Static Batching

Traditional serving:
```python
# Wait for batch to fill
batch = wait_for_requests(batch_size=32)

# Process entire batch
for step in range(max_steps):
    batch = model.forward(batch)

# All requests finish together
return batch
```

**Problems**:
1. **Head-of-line blocking**: Short requests wait for long ones
2. **Wasted GPU**: Finished requests still in batch (padding)
3. **Low throughput**: Can't add new requests until batch finishes

### 8.2 Continuous Batching: Dynamic Scheduling

**Key Idea**: Treat the batch as a **dynamic pool** that changes every step!

```python
# Request pool
waiting = []      # Not started yet
running = []      # Currently generating
finished = []     # Done!

while has_pending_requests():
    # 1. Remove finished requests
    for req in running:
        if req.is_finished:
            running.remove(req)
            finished.append(req)

    # 2. Add new requests (if space)
    while len(running) < max_batch_size and waiting:
        req = waiting.pop(0)
        running.append(req)

    # 3. Generate one token for all running requests
    batch = make_batch(running)
    outputs = model.forward(batch)

    # 4. Update requests with new tokens
    for req, output in zip(running, outputs):
        req.output_tokens.append(output)
```

**Benefits**:
1. **No wasted computation**: Remove finished immediately
2. **Better utilization**: Fill empty slots with new requests
3. **Lower latency**: Short requests don't wait for long ones

### 8.3 Implementation: RequestPool

```python
class RequestPool:
    def __init__(self):
        self.waiting = []
        self.running = []
        self.finished = []

    def add_requests(self, reqs):
        self.waiting.extend(reqs)

    def move_to_running(self, reqs):
        for req in reqs:
            if req in self.waiting:
                self.waiting.remove(req)
            if req not in self.running:
                self.running.append(req)

    def move_to_finished(self, reqs):
        for req in reqs:
            if req in self.running:
                self.running.remove(req)
            if req not in self.finished:
                self.finished.append(req)
                req.is_finished = True

    def has_pending(self):
        return len(self.waiting) > 0 or len(self.running) > 0
```

### 8.4 Implementation: ContinuousBatcher

```python
class ContinuousBatcher:
    def __init__(self, executor, radix_mgr, prefill_mgr, decode_mgr):
        self.executor = executor
        self.radix_mgr = radix_mgr
        self.prefill_mgr = prefill_mgr
        self.decode_mgr = decode_mgr
        self.request_pool = RequestPool()

    def step(self):
        """Execute one batching iteration"""

        # 1. Check for finished requests
        finished = [req for req in self.request_pool.running 
                    if self._is_finished(req)]
        self.request_pool.move_to_finished(finished)

        # Free KV cache
        for req in finished:
            self.radix_mgr.kv_pool.free(req.kv_pages)

        # 2. Schedule prefill (if space available)
        for req in list(self.request_pool.waiting):
            self.prefill_mgr.add_request(req)
            self.request_pool.waiting.remove(req)

        prefill_batch = self.prefill_mgr.schedule()
        if prefill_batch:
            logits = self.executor.execute_prefill(prefill_batch)
            self._process_prefill_output(prefill_batch, logits)
            self.request_pool.move_to_running(prefill_batch.requests)

            # Add to decode manager
            for req in prefill_batch.requests:
                self.decode_mgr.add_request(req)

        # 3. Schedule decode
        self.decode_mgr.remove_finished()
        decode_batch = self.decode_mgr.schedule()
        if decode_batch:
            logits = self.executor.execute_decode(decode_batch)
            self._process_decode_output(decode_batch, logits)

        return decode_batch.size if decode_batch else 0

    def run_until_complete(self, requests):
        """Run continuous batching until all requests finish"""
        self.request_pool.add_requests(requests)

        while self.request_pool.has_pending():
            self.step()

        return self.request_pool.finished
```

### 8.5 Example Timeline

```
Time 0: Requests [A, B, C] arrive
  Batch: [A, B, C] (all in prefill)

Time 1: Prefill completes
  Batch: [A, B, C] (all in decode)

Time 2: A finishes (short), D arrives
  Batch: [B, C, D] (D in prefill, B&C in decode)

Time 3: D finishes prefill
  Batch: [B, C, D] (all in decode)

Time 4: B finishes, E arrives
  Batch: [C, D, E] (E in prefill, C&D in decode)

...and so on!
```

**Key**: Batch composition changes every step!

### 8.6 Performance Impact

Continuous batching typically gives:
- **2-10x higher throughput** vs static batching
- **Lower latency** for short requests
- **Better GPU utilization** (80-95% vs 50-70%)

Used by: vLLM, TensorRT-LLM, SGLang, and all modern serving systems!

---

## 9. Phase 6: Complete Integration

### 9.1 Putting It All Together: NanoServingEngine

```python
class NanoServingEngine:
    def __init__(self, config, weights, kv_cache_pages=256):
        # Initialize KV cache pool
        self.kv_pool = KVCachePool(
            num_pages=kv_cache_pages,
            page_size=16,
            num_layers=config.n_layer,
            num_heads=config.n_head,
            head_dim=config.head_dim
        )

        # Initialize radix cache
        self.radix_mgr = RadixCacheManager(self.kv_pool)

        # Initialize schedulers
        self.prefill_mgr = ChunkedPrefillManager(self.kv_pool)
        self.decode_mgr = DecodeManager()

        # Initialize model executor
        self.executor = ModelExecutor(config, weights, self.kv_pool)

        # Initialize continuous batcher
        self.batcher = ContinuousBatcher(
            self.executor,
            self.radix_mgr,
            self.prefill_mgr,
            self.decode_mgr
        )

    def generate(self, prompt_tokens_list, sampling_params=None):
        """Main API: Generate completions"""
        # Create requests
        requests = [
            Request(
                req_id=i,
                prompt_tokens=tokens,
                max_tokens=params.max_tokens,
                temperature=params.temperature
            )
            for i, (tokens, params) in enumerate(
                zip(prompt_tokens_list, sampling_params)
            )
        ]

        # Run continuous batching
        finished = self.batcher.run_until_complete(requests)

        return finished

    def get_stats(self):
        """Get performance statistics"""
        return {
            'cache_hit_rate': self.radix_mgr.cache_hit_rate,
            'memory_utilization': 1.0 - (
                self.kv_pool.num_free_pages / self.kv_pool.num_pages
            ),
            'total_tokens_generated': self.batcher.total_tokens_generated,
            ...
        }
```

### 9.2 Usage Example

```python
# Initialize engine
engine = NanoServingEngine(
    config=ModelConfig(vocab_size=256, n_layer=2, ...),
    weights=load_weights("model.pt"),
    kv_cache_pages=256
)

# Generate completions
prompts = [
    [1, 2, 3, 4, 5],       # "What is MLIR"
    [1, 2, 3, 6, 7],       # "What is Python"
    [10, 20, 30]           # "Hello world"
]

params = [
    SamplingParams(max_tokens=10, temperature=0.7)
    for _ in prompts
]

# Run!
finished_requests = engine.generate(prompts, params)

for req in finished_requests:
    print(f"Input: {req.prompt_tokens}")
    print(f"Output: {req.output_tokens}")

# Check stats
stats = engine.get_stats()
print(f"Cache hit rate: {stats['cache_hit_rate']:.2%}")
print(f"Memory utilization: {stats['memory_utilization']:.2%}")
```

### 9.3 System Flow Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ User Requests                                   â”‚
â”‚ ["What is...", "How do...", "Explain..."]       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ NanoServingEngine.generate()                    â”‚
â”‚ - Tokenize prompts                              â”‚
â”‚ - Create Request objects                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ContinuousBatcher.run_until_complete()          â”‚
â”‚ Loop:                                           â”‚
â”‚   1. Check finished â†’ Free KV cache             â”‚
â”‚   2. Schedule prefill (via RadixCache)          â”‚
â”‚   3. Schedule decode                            â”‚
â”‚   4. Execute batches (ModelExecutor)            â”‚
â”‚   5. Sample tokens                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚                     â”‚
                   â–¼                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Prefill Path             â”‚  â”‚ Decode Path          â”‚
â”‚ - RadixCache lookup      â”‚  â”‚ - Batch running reqs â”‚
â”‚ - ChunkedPrefillManager  â”‚  â”‚ - DecodeManager      â”‚
â”‚ - Allocate KV pages      â”‚  â”‚ - Generate 1 token   â”‚
â”‚ - Execute prefill        â”‚  â”‚ - Update KV cache    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                              â”‚
         â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚      â”‚
         â–¼      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ModelExecutor (C++/MLIR)                        â”‚
â”‚ - GPT forward pass with MLIR JIT                â”‚
â”‚ - Read/write KV cache                           â”‚
â”‚ - Return logits                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Finished Requests                               â”‚
â”‚ [{req_id, prompt_tokens, output_tokens}, ...]   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 10. Performance Analysis

### 10.1 Test Results

From our Phase 6 tests:

```
Test 7: Throughput benchmark
  âœ“ Processed 20 requests
  âœ“ Total tokens: 160
  âœ“ Time: 0.008s
  âœ“ Throughput: 19,032 tokens/sec
```

For comparison:
- **Without batching**: ~100 tokens/sec (sequential)
- **With batching**: ~19,000 tokens/sec
- **Speedup**: 190x! ğŸš€

### 10.2 Cache Hit Rate Analysis

From Phase 4 tests:

```
Test 12: Realistic cache hit rate
  âœ“ Cache hit rate: 61.5%
  âœ“ Hits: 40, Misses: 25
```

**Interpretation**:
- 61.5% of tokens come from cache
- Means we compute only 38.5% of tokens
- Effective speedup: 1 / 0.385 = **2.6x faster**!

### 10.3 Memory Efficiency

Traditional approach (no paging):
```
3 users Ã— 1000 tokens Ã— 9.4 MB/token = 28.2 GB
```

Paged approach (16 tokens per page):
```
3 users Ã— 1000 tokens / 16 tokens/page Ã— page_size = 1.76 GB
Pages shared via radix cache â†’ ~1.0 GB actual usage
```

**Savings**: 28x less memory! ğŸ’°

### 10.4 Scalability

| Metric | Value | Impact |
|--------|-------|--------|
| **Batch Size** | 32 | 32x parallelism |
| **Cache Hit Rate** | 40-60% | 2-3x speedup |
| **Continuous Batching** | Dynamic | 2-5x utilization |
| **Paged Memory** | 16 tok/page | 10-30x capacity |
| **Combined Speedup** | 100-500x | vs naive sequential |

---

## 11. Comparison with Production Systems

### 11.1 vLLM (UC Berkeley)

**Our Implementation** â†’ **vLLM**

| Feature | Nano-Serving | vLLM |
|---------|--------------|------|
| **Paged KV Cache** | âœ… 16 tok/page | âœ… 16 tok/page |
| **Continuous Batching** | âœ… Simple | âœ… Advanced (with preemption) |
| **Prefix Caching** | âœ… Radix tree | âŒ (added later) |
| **GPU Support** | âŒ CPU only | âœ… CUDA kernels |
| **Performance** | ~20K tok/s | ~100K tok/s |

**Key Difference**: vLLM uses highly optimized CUDA kernels (PagedAttention).

### 11.2 SGLang (LMSYS)

**Our Implementation** â†’ **SGLang**

| Feature | Nano-Serving | SGLang |
|---------|--------------|--------|
| **Radix Attention** | âœ… Radix tree | âœ… Radix tree |
| **Cache Hit Rate** | 40-60% | 40-70% |
| **Continuous Batching** | âœ… Dynamic | âœ… Dynamic |
| **Constrained Decoding** | âŒ | âœ… FSM-based |
| **Performance** | ~20K tok/s | ~150K tok/s |

**Key Difference**: SGLang adds grammar-constrained generation and better GPU optimizations.

### 11.3 TensorRT-LLM (NVIDIA)

**Our Implementation** â†’ **TensorRT-LLM**

| Feature | Nano-Serving | TensorRT-LLM |
|---------|--------------|--------------|
| **Chunked Prefill** | âœ… 256 tok/chunk | âœ… Configurable |
| **Paged Attention** | âœ… Basic | âœ… Flash-Decoding |
| **Multi-GPU** | âŒ | âœ… Tensor parallel |
| **Quantization** | âŒ | âœ… INT4/INT8 |
| **Performance** | ~20K tok/s | ~300K tok/s |

**Key Difference**: TensorRT-LLM has CUDA graph optimization and hardware-specific tuning.

### 11.4 What We Learned

Our implementation captures **the core algorithms** used by production systems:
1. âœ… Paged memory management
2. âœ… Continuous batching
3. âœ… Prefix caching with radix trees
4. âœ… Chunked prefill for long contexts

The **100x+ speedup** is primarily about:
- Hardware: GPU vs CPU (10x)
- Optimizations: CUDA kernels, quantization, fusions (10x)

But the **algorithmic insights** are the same! ğŸ¯

---

## 12. Exercises & Extensions

### 12.1 Beginner Exercises

1. **Modify Batch Size**: Change `max_batch_size` and measure throughput impact
2. **Tune Chunk Size**: Experiment with different prefill chunk sizes (128, 256, 512)
3. **Add Metrics**: Track average latency per request
4. **Visualize Cache**: Print radix tree structure

### 12.2 Intermediate Exercises

1. **Implement Beam Search**: Extend sampling to support beam search
2. **Add Priority Queues**: Prioritize requests by SLA/latency requirements
3. **Memory Pressure**: Implement smarter eviction (not just LRU)
4. **Profile Bottlenecks**: Use profilers to find performance hotspots

### 12.3 Advanced Projects

1. **GPU Port**: Implement with PyTorch/CUDA for real GPU acceleration
2. **Multi-Node**: Extend to distributed serving with tensor parallelism
3. **Speculative Decoding**: Add draft model for faster generation
4. **Quantization**: Add INT8/INT4 quantization for memory efficiency
5. **Structured Output**: Implement grammar-constrained generation (like SGLang)

### 12.4 Research Extensions

1. **Adaptive Chunking**: Dynamically adjust chunk size based on load
2. **Prefix Scheduling**: Reorder requests to maximize cache hits
3. **Hybrid Caching**: Combine radix cache with semantic similarity
4. **Energy Optimization**: Schedule to minimize power consumption
5. **Multi-Modal**: Extend to handle images + text (LLaVA style)

---

## 13. Key Takeaways

### 13.1 Core Concepts

1. **LLM serving is fundamentally about batching**
   - Sequential: 100 tok/s
   - Batched: 20,000 tok/s (200x speedup)

2. **Memory is the bottleneck, not compute**
   - KV cache grows linearly with sequence length
   - Paged memory enables 10-30x more capacity

3. **Caching past computations is crucial**
   - 40-60% cache hit rate in practice
   - 2-3x effective speedup from prefix sharing

4. **Dynamic scheduling beats static batching**
   - Continuous batching: 2-5x better utilization
   - Chunked prefill: fairness for long contexts

### 13.2 Design Principles

1. **Separate control plane (Python) from data plane (C++/GPU)**
   - Flexibility + Performance

2. **Use virtual memory ideas for KV cache**
   - Paging + reference counting + LRU

3. **Prefix sharing via data structures, not heuristics**
   - Radix trees automatically find shared prefixes

4. **Balance throughput and latency**
   - Prefill vs decode scheduling
   - Token budgets
   - Chunking

### 13.3 Real-World Impact

Modern serving systems (vLLM, SGLang, TensorRT-LLM) all use these techniques:
- **100-1000x** faster than naive implementations
- **10-100x** higher capacity per GPU
- **Billions of requests** served daily

**The techniques you learned here power ChatGPT, Claude, Gemini, and every major LLM API!** ğŸŒŸ

---

## 14. Further Reading

### Papers
1. **vLLM**: "Efficient Memory Management for Large Language Model Serving with PagedAttention" (2023)
2. **SGLang**: "SGLang: Efficient Execution of Structured Language Model Programs" (2024)
3. **Orca**: "A Distributed Serving System for Transformer-Based Generative Models" (2022)

### Codebases
1. vLLM: https://github.com/vllm-project/vllm
2. SGLang: https://github.com/sgl-project/sglang
3. TensorRT-LLM: https://github.com/NVIDIA/TensorRT-LLM

### Courses
1. "Large Language Models" - Stanford CS324
2. "MLSys: The New Frontier of Machine Learning Systems" - Berkeley CS294